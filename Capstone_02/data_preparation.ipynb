{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for analysis.\n",
    "\n",
    "We are going to work with data provided by a company that connects different clients to different firms. More specifically, the company allows clients to book (schedule) visits to firms in order to learn more about their production processes and final products. Each booking has an unique identification ('b_id') which is used as index in the data structure. For confidentiality reason, some information cannot and will not be made public. To start, let's import the relevant modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relevant modules.\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data was stored in four different sheets inside a microsoft excel file named 'data_original'. That excel file was lightly organized and saved as a microsoft excel file named 'data_organized'. From 'data_organized' came three csv files: 'bookings_questions_data', 'bookings_addons_data' and 'main_data'. From the internet came other three csv files with necessary additional information: 'countries_data', 'us_regions_data' and 'us_zipcodes_data'. In summary, we are going to work with the following six csv files in the 'data' directory:\n",
    "\n",
    "- countries_data.csv\n",
    "- us_regions_data.csv\n",
    "- us_zipcodes_data.csv\n",
    "- bookings_questions_data.csv\n",
    "- bookings_addons_data.csv\n",
    "- main_data.csv\n",
    "\n",
    "In particular, the source of 'us_zipcodes_data.csv' is https://simple.wikipedia.org/wiki/List_of_ZIP_Code_prefixes.\n",
    "\n",
    "First, we are going to work each file individually, starting with 'countries_data.csv', which contains the abbreviation code and the full name of several countries. The end product of the code cell will be:\n",
    "\n",
    "- country_dic: dictionary with abbreviation code (key) and full name (value) for each country.\n",
    "- country_indic: inverted 'country_dic' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading countries_data.csv as pandas dataframe countries_df \n",
    "countries_df = pd.read_csv('data/countries_data.csv',\n",
    "                           index_col='country_abb', \n",
    "                           header=0, \n",
    "                           low_memory=False,\n",
    "                           keep_default_na=False,\n",
    "                           na_values='')\n",
    "\n",
    "## Checking for completely empty rows.\n",
    "if len(countries_df)!=len(countries_df.dropna(how='all')):\n",
    "    print('Attention! There are completely empty rows!')\n",
    "\n",
    "## Lower case for 'country'.\n",
    "countries_df['country'] = countries_df['country'].str.lower()\n",
    "\n",
    "## Creating dictionary and inverted dictionary.\n",
    "country_dic = countries_df['country'].to_dict()\n",
    "country_indic = {j: i for i, j in country_dic.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work with 'us_regions_data.csv', which contains the abbreviation code and the full name of all US states and territories. The end product of the code cell will be:\n",
    "\n",
    "- us_region_dic: dictionary with abbreviation code (key) and full name (value) for each US state or territory.\n",
    "- us_region_indic: inverted 'us_region_dic' dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading us_regions_data.csv as pandas dataframe usregions_df.\n",
    "usregions_df = pd.read_csv('data/us_regions_data.csv', \n",
    "                     index_col='region_abb', \n",
    "                     header=0, \n",
    "                     low_memory=False,\n",
    "                     keep_default_na=False,\n",
    "                     na_values='')\n",
    "\n",
    "## Checking for completely empty rows.\n",
    "if len(usregions_df)!=len(usregions_df.dropna(how='all')):\n",
    "    print('Attention! There are completely empty rows!')\n",
    "\n",
    "## Lower case and no left or right spaces for 'region'.\n",
    "usregions_df['region'] = usregions_df['region'].str.lower()\n",
    "usregions_df['region'] = usregions_df['region'].str.strip()\n",
    "\n",
    "## Creating dictionary and inverted dictionary.\n",
    "us_region_dic = usregions_df['region'].to_dict()\n",
    "us_region_indic = {j: i for i, j in us_region_dic.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work with 'us_zipcodes_data.csv', which contains the zipcode prefixes and the abbreviation code of US states and territories. The end product of the code cell will be:\n",
    "\n",
    "- us_zipcode_dic: dictionary with zipcode prefixes (key) and abbreviation code (value) of US states or territories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading us_zipcodes_data.csv as pandas dataframe uszipcodes_df.\n",
    "uszipcodes_df = pd.read_csv('data/us_zipcodes_data.csv',  \n",
    "                            converters={'zip_prefix': lambda x: str(x)},\n",
    "                            header=0, \n",
    "                            low_memory=False,\n",
    "                            keep_default_na=False,\n",
    "                            na_values='')\n",
    "\n",
    "## Checking for completely empty rows.\n",
    "if len(uszipcodes_df)!=len(uszipcodes_df.dropna(how='all')):\n",
    "    print('Attention! There are completely empty rows!')\n",
    "\n",
    "## Setting 'zip_prefix' as index.\n",
    "uszipcodes_df.set_index('zip_prefix', inplace=True)\n",
    "\n",
    "## Creating dictionary.\n",
    "us_zipcode_dic = uszipcodes_df['region'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work with 'bookings_questions_data.csv'. This file contains extra four categorical questions associated with some bookings. They will be named as questions 'q4' to 'q7'. The end product of the code cell will be:\n",
    "\n",
    "- questions_df: dataframe with booking id and columns for questions 'q4' to 'q7'.\n",
    "- q4_dic: dictionary with 'q4' and numerical codes (0 to 1) as keys, and descriptions as values. \n",
    "- q5_dic: dictionary with 'q5' and numerical codes (1 to 3) as keys, and descriptions as values. \n",
    "- q6_dic: dictionary with 'q6' and numerical codes (1 to 1684) as keys, and descriptions as values. \n",
    "- q7_dic: dictionary with 'q7' and numerical codes (1 to 5) as keys, and descriptions as values. \n",
    "\n",
    "Numerical codes 0 or 1 denote 'no' or 'yes' answers. Numerical codes starting with 1 denote descending categorical responses based on frequency (i.e., 1 meaning the most frequent answer). Question 'q6' is problematic because it has 1684 unique answers. Dictionaries 'q5_dic' and 'q6_dic' will not be made publicly available for confidentiality reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Reading bookings_questions_data.csv as pandas dataframe questions_df.\n",
    "questions_df = pd.read_csv('data/bookings_questions_data.csv', \n",
    "                           index_col='b_id', \n",
    "                           header=0, \n",
    "                           low_memory=False,\n",
    "                           keep_default_na=False,\n",
    "                           na_values='')\n",
    "\n",
    "## Checking for completely empty rows.\n",
    "if len(questions_df)!=len(questions_df.dropna(how='all')):\n",
    "    print('Attention! There are completely empty rows!')\n",
    "    \n",
    "## Lower case for all.\n",
    "for i in list(questions_df.columns):\n",
    "    questions_df[i] = questions_df[i].str.lower()\n",
    "\n",
    "## Fixing strings in rows that will become columns.\n",
    "q_list = list(questions_df['b_extra_q'].value_counts(ascending=False).index)\n",
    "questions_df['b_extra_q'].replace([q_list[2],q_list[4]],\n",
    "                                  'q4 '+q_list[2], inplace=True)\n",
    "questions_df['b_extra_q'].replace(q_list[0],\n",
    "                                  'q5 '+q_list[0], inplace=True)\n",
    "questions_df['b_extra_q'].replace(q_list[1],\n",
    "                                  'q6 '+q_list[1], inplace=True)\n",
    "questions_df['b_extra_q'].replace(q_list[3],\n",
    "                                  'q7 '+q_list[3]+'?', inplace=True)\n",
    "\n",
    "## Making each question a column with its answer.\n",
    "questions_df = questions_df.pivot(columns='b_extra_q', values='b_extra_a')\n",
    "questions_df.columns.name = None\n",
    "\n",
    "## Coding questions q4-q7 and creating dictionary for them: q#_dic.\n",
    "for i in list(questions_df.columns):\n",
    "    if i.startswith('q4'):\n",
    "        questions_df.loc[questions_df[i].str.startswith('yes', na=False),i]=1\n",
    "        questions_df.loc[questions_df[i].str.startswith('no', na=False),i]=0\n",
    "        questions_df[i] = questions_df[i].astype('Int64')\n",
    "        q4_dic = {'question': i[3:], 0: 'no', 1: 'yes'}\n",
    "        questions_df.rename(columns={i: i[0:2]}, inplace=True)\n",
    "    elif i.startswith('q5'):\n",
    "        j = list(questions_df[i].value_counts(ascending=False).index)\n",
    "        jp = list(range(1, len(j)+1))\n",
    "        zipped = list(zip(jp, j))\n",
    "        zipped.insert(0, ('question', i[3:]))\n",
    "        q5_dic = dict(zipped) \n",
    "        q5_indic = {value: key for key, value in q5_dic.items()}\n",
    "        questions_df[i] = questions_df[i].map(q5_indic).astype('Int64')\n",
    "        questions_df.rename(columns={i: i[0:2]}, inplace=True)\n",
    "    elif i.startswith('q6'):\n",
    "        j = list(questions_df[i].value_counts(ascending=False).index)\n",
    "        jp = list(range(1, len(j)+1))\n",
    "        zipped = list(zip(jp, j))\n",
    "        zipped.insert(0, ('question', i[3:]))\n",
    "        q6_dic = dict(zipped) \n",
    "        q6_indic = {value: key for key, value in q6_dic.items()}\n",
    "        questions_df[i] = questions_df[i].map(q6_indic).astype('Int64')\n",
    "        questions_df.rename(columns={i: i[0:2]}, inplace=True)\n",
    "    elif i.startswith('q7'):\n",
    "        j = list(questions_df[i].value_counts(ascending=False).index)\n",
    "        jp = list(range(1, len(j)+1))\n",
    "        zipped = list(zip(jp, j))\n",
    "        zipped.insert(0, ('question', i[3:]))\n",
    "        q7_dic = dict(zipped) \n",
    "        q7_indic = {value: key for key, value in q7_dic.items()}\n",
    "        questions_df[i] = questions_df[i].map(q7_indic).astype('Int64')\n",
    "        questions_df.rename(columns={i: i[0:2]}, inplace=True)\n",
    "    else:\n",
    "        print('Attention! Something is wrong!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will work with 'bookings_addons_data.csv'. This file contains add-ons purchases associated with some bookings. It informs price, quantity and revenue (price x quantity) of 27 different add-ons. It also informs total quantity and revenue corresponding to all add-ons. The end product of the code cell will be:\n",
    "\n",
    "- addons_df: dataframe with booking id and columns related to 'addon_01' through 'addon_27'.\n",
    "- addons_dic: dictionary with 'addon_01' through 'addon_27' as keys, and their descriptions as values.\n",
    "\n",
    "Add-ons can be, for example, souvenirs or special additions to the booking. The dictionary 'addons_dic' wil not be made publicly available for confidentiality reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading bookings_addons_data.csv as pandas dataframe addons_df.\n",
    "addons_df = pd.read_csv('data/bookings_addons_data.csv',\n",
    "                        index_col='b_id', \n",
    "                        header=0,\n",
    "                        low_memory=False,\n",
    "                        keep_default_na=False,\n",
    "                        na_values='')\n",
    "\n",
    "## Dropping completely empty rows and fixing types.\n",
    "addons_df.dropna(how='all', inplace=True)\n",
    "addons_df.index = addons_df.index.map(int)\n",
    "addons_df['b_addons_quant'] = addons_df['b_addons_quant'].astype('int64')\n",
    "\n",
    "## Group by addons names.\n",
    "agg_dic = {'b_addons_price':np.average,\n",
    "           'b_addons_quant':np.sum,\n",
    "           'b_addons_rev':np.sum}\n",
    "addons_name_df = addons_df.groupby(['b_addons_name']).agg(agg_dic)\n",
    "\n",
    "## Sort addons names first by quantity and then by revenue.\n",
    "addons_name_df.sort_values(['b_addons_quant', 'b_addons_rev'],\n",
    "                           ascending=[False, False],\n",
    "                           inplace=True)\n",
    "\n",
    "## Creating addons codes for addons names based on the previous sorting.\n",
    "addons_name_df['b_addons_code'] = ['addon_'+f'{i:02d}' for i in \n",
    "                                   np.arange(1,len(addons_name_df)+1)]\n",
    "\n",
    "## Creating dictionary to connect addons names to addons codes.\n",
    "addons_dic = addons_name_df['b_addons_code'].to_dict()\n",
    "\n",
    "## Adding a column of addons codes to the original dataframe addons_df.\n",
    "addons_df['b_addons_code'] = addons_df['b_addons_name'].map(addons_dic)\n",
    "addons_df = addons_df[['b_addons_code',\n",
    "                       'b_addons_name',\n",
    "                       'b_addons_price',\n",
    "                       'b_addons_quant',\n",
    "                       'b_addons_rev']]\n",
    "\n",
    "## Remaking dictionary so addons codes give addons names if necessary.\n",
    "addons_code_df = pd.DataFrame.from_dict(addons_dic.items())\n",
    "addons_code_df.columns = ['b_addons_name','b_addons_code']\n",
    "addons_code_df.set_index('b_addons_code', inplace=True)\n",
    "addons_dic = addons_code_df['b_addons_name'].to_dict()\n",
    "\n",
    "## Dropping column addons name for confidentiality reason.\n",
    "addons_df.drop(columns=['b_addons_name'], inplace=True)\n",
    "\n",
    "## Making each addon a column with its price, quantity and revenue.\n",
    "addons_df = addons_df.pivot(columns='b_addons_code', \n",
    "                            values=['b_addons_price',\n",
    "                                    'b_addons_quant',\n",
    "                                    'b_addons_rev'])\n",
    "\n",
    "## Flattening hierarchical columns.\n",
    "addons_df.columns = [i[1]+'_'+i[0][9:]\n",
    "                     for i in list(addons_df.columns)]\n",
    "\n",
    "## Creating column with total quantity of addons.\n",
    "col_list = list(filter(lambda x: x[7:8].isdigit() and x.endswith('_quant'),\n",
    "                       addons_df.columns))\n",
    "addons_df['tot_quant_addons_alt'] = addons_df[col_list].sum(axis=1, skipna=True)\n",
    "\n",
    "## Creating column with total revenue from addons.\n",
    "col_list = list(map(lambda x: x+'_rev', \n",
    "                    list(addons_dic.keys())))\n",
    "addons_df['tot_rev_addons_alt'] = addons_df[col_list].sum(axis=1, skipna=True)\n",
    "\n",
    "## Checking if price * quant = revenue for each addon.\n",
    "for i in list(addons_dic.keys()):\n",
    "    rev_a = addons_df[i+'_rev'].dropna().round(10)\n",
    "    rev_b = addons_df[i+'_price']*addons_df[i+'_quant']\n",
    "    rev_b = rev_b.dropna().round(10)\n",
    "    if set(rev_a) != set(rev_b):\n",
    "        print('Checking if price * quant = revenue for each addon:')\n",
    "        print('Problem with', i)\n",
    "        print('Is size the problem?', len(rev_a)!=len(rev_b))\n",
    "        print('Where is the problem?')\n",
    "        for j in zip(rev_a,rev_b):\n",
    "            if j[0] != j[1]:\n",
    "                print(f'{j[0]:0.20f}', f'{j[1]:0.20f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will work with 'main_data.csv'. As the name suggests, this is the main file with several different information associate with all bookings between 2016-08-31 and 2017-12-31. There is information about the client booking a visit, the booking itself, and the firm to be visited. The end product of the code cell will be:\n",
    "\n",
    "- main_df: dataframe with booking id and several columns of information. Some columns have self-explanatory titles. Other columns will have dictionaries with descriptions (see next items).\n",
    "- firm_id_dic: dictionary with each firm's numerical code as key, and each firm's name as value.\n",
    "- firm_type_dic: dictionary with numerical codes (1 to 3) as keys, and descriptions as values. \n",
    "- status_cancel_dic: dictionary with numerical codes (0 to 1) as keys, and descriptions as values.\n",
    "- source_online_dic: dictionary with numerical codes (0 to 1) as keys, and descriptions as values.\n",
    "- payment_cc_dic: dictionary with numerical codes (-1 to 1) as keys, and descriptions as values.\n",
    "- discount_dic: dictionary with numerical codes (0 to 1) as keys, and descriptions as values.\n",
    "- q1_bef_dic: dictionary with 'q1_bef' as key, and its description as value.\n",
    "- q1_aft_dic: dictionary with 'q1_aft' as key, and its description as value.\n",
    "- q2_dic: dictionary with 'q2' and numerical codes (0 to 3) as keys, and descriptions as values.\n",
    "- q3_dic: dictionary with 'q3' and numerical codes (0 to 1) as keys, and descriptions as values.\n",
    "    \n",
    "Basically, for those columns with dictionaries, the reader can access '[insert column's title]_dic' to read descriptions. However, the dictionaries 'firm_id_dic' and 'firm_type_dic' will not be made publicly available for confidentiality reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading main_data.csv as pandas dataframe main_df.\n",
    "main_df = pd.read_csv('data/main_data.csv', \n",
    "                      index_col='b_id', \n",
    "                      header=0, \n",
    "                      low_memory=False,\n",
    "                      keep_default_na=False,\n",
    "                      na_values='')\n",
    "\n",
    "## Checking for completely empty rows.\n",
    "if len(main_df)!=len(main_df.dropna(how='all')):\n",
    "    print('Attention! There are completely empty rows!')\n",
    "\n",
    "## Fixing names of columns.\n",
    "col_list = list(main_df.columns.values)\n",
    "new_col_list = []\n",
    "for i in col_list:\n",
    "    if i[0:2] == 'b_':\n",
    "        new_col_list.append(i[2:])\n",
    "    elif i[0:2] == 'f_':\n",
    "        new_col_list.append('firm_'+i[2:])\n",
    "    elif i[0:2] == 'p_':\n",
    "        new_col_list.append('visit_'+i[2:])\n",
    "    else: print('Attention! Something is missing!')\n",
    "main_df.columns = new_col_list\n",
    "        \n",
    "## Fixing types of some columns.\n",
    "main_df['age'] = main_df['age'].astype('Int64')\n",
    "main_df['guests_in'] = main_df['guests_in'].astype('Int64')\n",
    "main_df['nps_bef'] = main_df['nps_bef'].astype('Int64')\n",
    "main_df['nps_aft'] = main_df['nps_aft'].astype('Int64')\n",
    "main_df['birthday'] = pd.to_datetime(main_df['birthday'])\n",
    "main_df['date'] = pd.to_datetime(main_df['date'])\n",
    "main_df.rename(columns={'date': 'contact_date'}, inplace=True)\n",
    "main_df.insert(main_df.columns.get_loc('firm_loc')+1, 'contact_date',\n",
    "               main_df.pop('contact_date'))\n",
    "\n",
    "## Creating 'visit_datetime' from 'visit_date' and 'visit_time'.\n",
    "main_df['visit_datetime'] = main_df['visit_date']+' '+main_df['visit_time']\n",
    "main_df['visit_datetime'] = pd.to_datetime(main_df['visit_datetime'])\n",
    "main_df.insert(main_df.columns.get_loc('visit_name')+1, 'visit_datetime',\n",
    "               main_df.pop('visit_datetime'))\n",
    "main_df.drop(columns=['visit_date', 'visit_time'], inplace=True) \n",
    "\n",
    "## Dropping some columns for confidentiality reasons.\n",
    "main_df.drop(columns=['birthday', 'address', 'visit_name'], inplace=True)\n",
    "\n",
    "## Creating dictionary with 'firm_id' keys and 'firm_name' values: firm_id_dic.\n",
    "firm_name_df = main_df[['firm_id', 'firm_name']].groupby('firm_name').mean()\n",
    "firm_name_dic = firm_name_df['firm_id'].to_dict()\n",
    "firm_id_dic = {j: i for i, j in firm_name_dic.items()}\n",
    "main_df.drop(columns='firm_name', inplace=True)\n",
    "\n",
    "## Coding 'firm_type' and creating dictionary for it: firm_type_dic.\n",
    "main_df['firm_type']=main_df['firm_type'].str.lower()\n",
    "types_list = list(main_df['firm_type'].value_counts(ascending=False).index)\n",
    "firm_type_dic = {1: types_list[0], 2: types_list[1], 3: types_list[2]}\n",
    "firm_type_indic = {j: i for i, j in firm_type_dic.items()} \n",
    "main_df['firm_type'] = main_df['firm_type'].map(firm_type_indic)\n",
    "\n",
    "## Coding 'status_cancel' and creating dictionary for it: status_cancel_dic.\n",
    "main_df.rename(columns={'visit_status': 'status_cancel'}, inplace=True)\n",
    "main_df['status_cancel'] = main_df['status_cancel'].str.lower()\n",
    "status_cancel_dic = {0: 'confirmed', 1: 'cancelled'}\n",
    "status_cancel_indic = {j: i for i, j in status_cancel_dic.items()} \n",
    "main_df['status_cancel'] = main_df['status_cancel'].map(status_cancel_indic)\n",
    "\n",
    "## Coding 'source_online' and creating dictionary for it: source_online_dic.\n",
    "main_df.rename(columns={'source': 'source_online'}, inplace=True)\n",
    "main_df['source_online'] = main_df['source_online'].str.lower()\n",
    "source_online_dic = {0: 'added', 1: 'online'}\n",
    "source_online_indic = {j: i for i, j in source_online_dic.items()} \n",
    "main_df['source_online'] = main_df['source_online'].map(source_online_indic)\n",
    "\n",
    "## Coding 'payment_cc' and creating dictionary for it: payment_cc_dic.\n",
    "main_df.rename(columns={'payment': 'payment_cc'}, inplace=True)\n",
    "main_df['payment_cc'] = main_df['payment_cc'].str.lower()\n",
    "payment_cc_dic = {-1: 'none', 0: 'cash', 1: 'credit card'}\n",
    "payment_cc_indic = {j: i for i, j in payment_cc_dic.items()} \n",
    "main_df['payment_cc'] = main_df['payment_cc'].map(payment_cc_indic)\n",
    "\n",
    "## Coding 'discount' and creating dictionary for it: discount_dic.\n",
    "main_df['discount'] = main_df['discount'].str.lower()\n",
    "discount_dic = {0: 'no', 1: 'yes'}\n",
    "main_df.loc[main_df['discount'].notna(), 'discount'] = 1\n",
    "main_df.loc[main_df['discount'].isna(), 'discount'] = 0\n",
    "\n",
    "## Organizing question 1 (q1_bef & q1_aft) and its dictionary (q1_bef_dic & q1_aft_dic).\n",
    "q1_bef_dic = {'question': 'likelihood to recommend our product before visit on scale 0-10'} \n",
    "q1_aft_dic = {'question': 'likelihood to recommend our product after visit on scale 0-10'}\n",
    "main_df.rename(columns = {'nps_bef': 'q1_bef', 'nps_aft': 'q1_aft'}, \n",
    "               inplace = True) \n",
    "main_df.insert(main_df.columns.get_loc('tot_rev')+1, 'q1_bef', main_df.pop('q1_bef'))\n",
    "main_df.insert(main_df.columns.get_loc('q1_bef')+1, 'q1_aft', main_df.pop('q1_aft'))\n",
    "main_df.drop(columns ='nps_q', inplace=True) \n",
    "\n",
    "## Organizing question 2 (q2) and its dictionary (q2_dic).\n",
    "main_df['behavior_a'] = main_df['behavior_a'].str.lower()\n",
    "main_df['behavior_a'].replace('at least once a year', 'occasionally', inplace=True)\n",
    "q2_dic = {'question': 'how often did you use our product in the past year?', \n",
    "          0: 'never',\n",
    "          1: 'occasionally',\n",
    "          2: 'at least once a month', \n",
    "          3: 'at least once a week'}\n",
    "q2_indic = {j: i for i, j in q2_dic.items()} \n",
    "main_df['q2'] = main_df['behavior_a'].map(q2_indic).astype('Int64')\n",
    "main_df.insert(main_df.columns.get_loc('q1_aft')+1, 'q2', main_df.pop('q2'))\n",
    "main_df.drop(columns =['behavior_q', 'behavior_a'], inplace=True) \n",
    "\n",
    "## Organizing question 3 (q3) and its dictionary (q3_dic).\n",
    "q3_dic = {'question': 'would you join our email list?', \n",
    "          0: 'no', \n",
    "          1: 'yes'}\n",
    "main_df.loc[main_df['email_a'].str.contains('I don', na=False), 'q3'] = 0\n",
    "main_df.loc[main_df['email_a'].str.contains('events', na=False), 'q3'] = 1\n",
    "main_df['q3'] = main_df['q3'].astype('Int64')\n",
    "main_df.insert(main_df.columns.get_loc('q2')+1, 'q3', main_df.pop('q3'))\n",
    "main_df.drop(columns =['email_q', 'email_a'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the next code cell, we are going to:\n",
    "\n",
    "1-) make a copy of 'main.df' called 'final_df'.  \n",
    "2-) use the dictionary 'country_dic' on the dataframe 'final_df' to make the column 'country' show the full name of each country.  \n",
    "3-) use the dictionaries 'us_region_dic', 'us_region_indic' and 'us_zipcode_dic' on the dataframe 'final_df' to create columns 'us_region' and 'us_zipcode'.  \n",
    "4-) merge the dataframe 'questions_df' to the dataframe 'final_df', and organizing it.  \n",
    "5-) merge the dataframe 'addons_df' to the dataframe 'final_df', and organizing it.  \n",
    "\n",
    "The goal is to have all information included in a single dataframe: 'final_df'.  \n",
    "All dictionaries will be preserved in 'final_dic' (a dictionary of dictionaries).  \n",
    "All publicly available dictionaries will be preserved in 'final_dic_pub' (a dictionary of dictionaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 56640 entries, 132755 to 295082\n",
      "Data columns (total 112 columns):\n",
      "age                 9051 non-null Int64\n",
      "age_range           9051 non-null object\n",
      "country             41135 non-null object\n",
      "us_region           40599 non-null object\n",
      "us_zipcode          40529 non-null object\n",
      "firm_id             56425 non-null float64\n",
      "firm_type           56425 non-null float64\n",
      "firm_loc            56425 non-null object\n",
      "contact_date        56425 non-null datetime64[ns]\n",
      "visit_datetime      56425 non-null datetime64[ns]\n",
      "status_cancel       56425 non-null float64\n",
      "source_online       56425 non-null float64\n",
      "payment_cc          56425 non-null float64\n",
      "guests              56425 non-null float64\n",
      "guests_in           34521 non-null Int64\n",
      "fee                 56425 non-null float64\n",
      "discount            56425 non-null float64\n",
      "tot_quant_addons    56425 non-null float64\n",
      "tot_rev_addons      56425 non-null float64\n",
      "tot_rev             56425 non-null float64\n",
      "q1_bef              35154 non-null Int64\n",
      "q1_aft              16412 non-null Int64\n",
      "q2                  35437 non-null Int64\n",
      "q3                  10561 non-null Int64\n",
      "feedback            5313 non-null object\n",
      "q4                  1355 non-null Int64\n",
      "q5                  5978 non-null Int64\n",
      "q6                  5961 non-null Int64\n",
      "q7                  247 non-null Int64\n",
      "merge_questions     56640 non-null object\n",
      "addon_01_price      1624 non-null float64\n",
      "addon_02_price      545 non-null float64\n",
      "addon_03_price      55 non-null float64\n",
      "addon_04_price      291 non-null float64\n",
      "addon_05_price      226 non-null float64\n",
      "addon_06_price      169 non-null float64\n",
      "addon_07_price      17 non-null float64\n",
      "addon_08_price      93 non-null float64\n",
      "addon_09_price      14 non-null float64\n",
      "addon_10_price      98 non-null float64\n",
      "addon_11_price      37 non-null float64\n",
      "addon_12_price      50 non-null float64\n",
      "addon_13_price      88 non-null float64\n",
      "addon_14_price      90 non-null float64\n",
      "addon_15_price      18 non-null float64\n",
      "addon_16_price      62 non-null float64\n",
      "addon_17_price      26 non-null float64\n",
      "addon_18_price      33 non-null float64\n",
      "addon_19_price      33 non-null float64\n",
      "addon_20_price      13 non-null float64\n",
      "addon_21_price      8 non-null float64\n",
      "addon_22_price      20 non-null float64\n",
      "addon_23_price      13 non-null float64\n",
      "addon_24_price      6 non-null float64\n",
      "addon_25_price      12 non-null float64\n",
      "addon_26_price      6 non-null float64\n",
      "addon_27_price      1 non-null float64\n",
      "addon_01_quant      1624 non-null float64\n",
      "addon_02_quant      545 non-null float64\n",
      "addon_03_quant      55 non-null float64\n",
      "addon_04_quant      291 non-null float64\n",
      "addon_05_quant      226 non-null float64\n",
      "addon_06_quant      169 non-null float64\n",
      "addon_07_quant      17 non-null float64\n",
      "addon_08_quant      93 non-null float64\n",
      "addon_09_quant      14 non-null float64\n",
      "addon_10_quant      98 non-null float64\n",
      "addon_11_quant      37 non-null float64\n",
      "addon_12_quant      50 non-null float64\n",
      "addon_13_quant      88 non-null float64\n",
      "addon_14_quant      90 non-null float64\n",
      "addon_15_quant      18 non-null float64\n",
      "addon_16_quant      62 non-null float64\n",
      "addon_17_quant      26 non-null float64\n",
      "addon_18_quant      33 non-null float64\n",
      "addon_19_quant      33 non-null float64\n",
      "addon_20_quant      13 non-null float64\n",
      "addon_21_quant      8 non-null float64\n",
      "addon_22_quant      20 non-null float64\n",
      "addon_23_quant      13 non-null float64\n",
      "addon_24_quant      6 non-null float64\n",
      "addon_25_quant      12 non-null float64\n",
      "addon_26_quant      6 non-null float64\n",
      "addon_27_quant      1 non-null float64\n",
      "addon_01_rev        1624 non-null float64\n",
      "addon_02_rev        545 non-null float64\n",
      "addon_03_rev        55 non-null float64\n",
      "addon_04_rev        291 non-null float64\n",
      "addon_05_rev        226 non-null float64\n",
      "addon_06_rev        169 non-null float64\n",
      "addon_07_rev        17 non-null float64\n",
      "addon_08_rev        93 non-null float64\n",
      "addon_09_rev        14 non-null float64\n",
      "addon_10_rev        98 non-null float64\n",
      "addon_11_rev        37 non-null float64\n",
      "addon_12_rev        50 non-null float64\n",
      "addon_13_rev        88 non-null float64\n",
      "addon_14_rev        90 non-null float64\n",
      "addon_15_rev        18 non-null float64\n",
      "addon_16_rev        62 non-null float64\n",
      "addon_17_rev        26 non-null float64\n",
      "addon_18_rev        33 non-null float64\n",
      "addon_19_rev        33 non-null float64\n",
      "addon_20_rev        13 non-null float64\n",
      "addon_21_rev        8 non-null float64\n",
      "addon_22_rev        20 non-null float64\n",
      "addon_23_rev        13 non-null float64\n",
      "addon_24_rev        6 non-null float64\n",
      "addon_25_rev        12 non-null float64\n",
      "addon_26_rev        6 non-null float64\n",
      "addon_27_rev        1 non-null float64\n",
      "merge_addons        56640 non-null object\n",
      "dtypes: Int64(10), datetime64[ns](2), float64(92), object(8)\n",
      "memory usage: 49.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## 1-) make a copy of 'main.df' called 'final_df'.\n",
    "final_df = main_df.copy()\n",
    "\n",
    "## 2-) use 'country_dic' on 'final_df'.\n",
    "final_df['country'] = final_df['country'].map(country_dic)\n",
    "\n",
    "## 3-) use 'us_region_dic', 'us_region_indic' and 'us_zipcode_dic' on 'final_df'.\n",
    "\n",
    "## Creating column 'us_zipcode'.\n",
    "f1 = final_df['country'] == 'united states'\n",
    "f2 = final_df['country'].isna()\n",
    "us_zip_list = []\n",
    "for i in final_df.loc[f1 | f2, 'zipcode'].astype(str).values:\n",
    "    i_c1 = i.strip().isdigit()\n",
    "    i_c2 = len(i.strip()) <= 5\n",
    "    i_c3 = '-' in i\n",
    "    i_c4 = i.split('-')[0].strip().isdigit()\n",
    "    i_c5 = len(i.split('-')[0].strip()) <= 5   \n",
    "    if i_c1 and i_c2:\n",
    "        j = f'{int(i.strip()):05d}'\n",
    "        us_zip_list.append(j)\n",
    "    elif i_c3 and i_c4 and i_c5:\n",
    "        j = f'{int(i.split(\"-\")[0].strip()):05d}'\n",
    "        us_zip_list.append(j)\n",
    "    else:\n",
    "        j = np.nan\n",
    "        us_zip_list.append(j)\n",
    "final_df.loc[f1 | f2, 'us_zipcode'] = us_zip_list\n",
    "final_df.insert(final_df.columns.get_loc('country')+1, \n",
    "                'us_zipcode', final_df.pop('us_zipcode'))\n",
    "\n",
    "## Creating column 'us_region'.\n",
    "f1 = final_df['country'] == 'united states'\n",
    "f2 = final_df['country'].isna()\n",
    "l1 = ['region', 'us_zipcode']\n",
    "us_region_list = []\n",
    "for i, j in final_df.loc[f1 | f2, l1].astype(str).values:\n",
    "    i_c1 = i.strip().upper() in us_region_dic\n",
    "    i_c2 = i.strip().lower() in us_region_indic\n",
    "    j_c1 = str(j)[0:3] in us_zipcode_dic\n",
    "    if i_c1 and j_c1:\n",
    "        if i.strip().upper() == us_zipcode_dic[str(j)[0:3]]:\n",
    "            k = i.strip().upper()\n",
    "            us_region_list.append(k)\n",
    "        else:\n",
    "            k = 'unclear'\n",
    "            us_region_list.append(k)\n",
    "    elif i_c2 and j_c1:\n",
    "        if us_region_indic[i.strip().lower()] == us_zipcode_dic[str(j)[0:3]]:\n",
    "            k = us_region_indic[i.strip().lower()]\n",
    "            us_region_list.append(k)\n",
    "        else:\n",
    "            k = 'unclear'\n",
    "            us_region_list.append(k)\n",
    "    elif i_c1:\n",
    "        k = i.strip().upper()\n",
    "        us_region_list.append(k)\n",
    "    elif i_c2:\n",
    "        k = us_region_indic[i.strip().lower()]\n",
    "        us_region_list.append(k)\n",
    "    elif j_c1:\n",
    "        k = us_zipcode_dic[str(j)[0:3]]\n",
    "        us_region_list.append(k)\n",
    "    else:\n",
    "        k = np.nan\n",
    "        us_region_list.append(k)\n",
    "final_df.loc[f1 | f2, 'us_region'] = us_region_list\n",
    "final_df.insert(final_df.columns.get_loc('country')+1, \n",
    "                'us_region', final_df.pop('us_region'))\n",
    "\n",
    "## Corrections and fine-tuning in columns 'country', 'us_region' and 'us_zipcode'.\n",
    "f1 = final_df['country'].isna()\n",
    "f2 = final_df['us_region'].notna()\n",
    "f3 = final_df['us_region'] != 'International Mail'\n",
    "f4 = final_df['us_region'] != 'NOT USED'\n",
    "final_df.loc[f1 & f2 & f3 & f4, 'country'] = 'united states'\n",
    "\n",
    "f1 = final_df['country'].isna()\n",
    "f2 = final_df['us_region'] == 'International Mail'\n",
    "f3 = final_df['us_region'] == 'NOT USED'\n",
    "final_df.loc[f1 & (f2 | f3), 'country'] = 'japan'\n",
    "final_df.loc[f1 & (f2 | f3), ['us_region', 'us_zipcode']] = [np.nan, np.nan] \n",
    "\n",
    "f1 = final_df['country'] == 'united states'\n",
    "f2 = final_df['us_region'].isna()\n",
    "f3 = final_df['us_region'] == 'International Mail'\n",
    "f4 = final_df['us_region'] == 'NOT USED'\n",
    "f5 = final_df['region'].notna()\n",
    "f6 = final_df['zipcode'].notna()\n",
    "final_df.loc[f1 & (f2 | f3 | f4) & (f5 | f6),\n",
    "             ['country', 'us_region', 'us_zipcode']] = ['unclear', np.nan, np.nan]\n",
    "\n",
    "f1 = final_df['country'].isna()\n",
    "f2 = final_df['region'].notna()\n",
    "f3 = final_df['zipcode'].notna()\n",
    "final_df.loc[f1 & (f2 | f3), 'country'] = 'canada'\n",
    "\n",
    "f1 = final_df['country'].notna()\n",
    "f2 = final_df['country'] != 'united states'\n",
    "f3 = final_df['country'] != 'unclear'\n",
    "final_df.loc[f1 & f2 & f3, ['us_region', 'us_zipcode']] = ['foreigner','foreigner']\n",
    "\n",
    "## Dropping now-unecessary columns for confidentiality reason.\n",
    "final_df.drop(columns=['region', 'city', 'zipcode'], inplace=True)\n",
    "\n",
    "## 4-) merge 'questions_df' to 'final_df'.\n",
    "final_df = pd.merge(final_df, questions_df, on='b_id', how='outer', indicator=True)\n",
    "final_df.rename(columns={'_merge': 'merge_questions'}, inplace=True)\n",
    "final_df['merge_questions'] = final_df['merge_questions'].astype(str)\n",
    "\n",
    "## 5-) merge 'addons_df' to 'final_df'.\n",
    "final_df = pd.merge(final_df, addons_df, on='b_id', how='outer', indicator=True)\n",
    "final_df.rename(columns={'_merge': 'merge_addons'}, inplace=True)\n",
    "final_df['merge_addons'] = final_df['merge_addons'].astype(str)\n",
    "\n",
    "## Filling up 'nan' in a column with available information in other column.\n",
    "for i in ['tot_quant_addons','tot_rev_addons']:\n",
    "    f1 = final_df[i].notna()\n",
    "    f2 = final_df[i+'_alt'].isna()\n",
    "    final_df.loc[f1 & f2, i+'_alt'] = final_df.loc[f1 & f2, i]\n",
    "    f3 = final_df[i].isna()\n",
    "    f4 = final_df[i+'_alt'].notna()\n",
    "    final_df.loc[f3 & f4, i] = final_df.loc[f3 & f4, i+'_alt']\n",
    "     \n",
    "## Correcting 'tot_quant_addons'.\n",
    "f1 = final_df['tot_quant_addons'] != final_df['tot_quant_addons_alt']\n",
    "f2 = final_df['tot_rev_addons'] == final_df['tot_rev_addons_alt']\n",
    "final_df.loc[f1 & f2, 'tot_quant_addons'] = final_df.loc[f1 & f2, 'tot_quant_addons_alt']\n",
    "\n",
    "## Correcting 'tot_rev_addons'.\n",
    "f1 = final_df['tot_quant_addons'] == final_df['tot_quant_addons_alt']\n",
    "f2 = final_df['tot_rev_addons'] != final_df['tot_rev_addons_alt']\n",
    "final_df.loc[f1 & f2, 'tot_rev_addons'] = final_df.loc[f1 & f2, 'tot_rev_addons_alt']\n",
    "\n",
    "## Correcting 'tot_quant_addons_alt' and 'tot_rev_addons_alt'.\n",
    "f1 = final_df['tot_quant_addons'] > final_df['tot_quant_addons_alt']\n",
    "f2 = final_df['tot_rev_addons'] > final_df['tot_rev_addons_alt']\n",
    "final_df.loc[f1 & f2, 'tot_quant_addons_alt'] = final_df.loc[f1 & f2, 'tot_quant_addons']\n",
    "final_df.loc[f1 & f2, 'tot_rev_addons_alt'] = final_df.loc[f1 & f2, 'tot_rev_addons']\n",
    "\n",
    "## Dropping now-unecessary columns and rounding 'tot_rev_addons'.\n",
    "final_df.drop(columns=['tot_quant_addons_alt', 'tot_rev_addons_alt'], inplace=True)\n",
    "final_df['tot_rev_addons'] = final_df['tot_rev_addons'].round(2)\n",
    "\n",
    "## Correcting and rounding 'tot_rev'.\n",
    "f1 = final_df['tot_rev'].isna()\n",
    "f2 = final_df['tot_rev_addons'].notna()\n",
    "f3 = final_df['tot_rev'] < final_df['tot_rev_addons']\n",
    "final_df.loc[(f1 & f2) | f3, 'tot_rev'] = final_df.loc[(f1 & f2) | f3, 'tot_rev_addons']\n",
    "final_df['tot_rev'] = final_df['tot_rev'].round(2)\n",
    "\n",
    "## Recalculating and rounding 'fee'.\n",
    "final_df['fee'] = (final_df['tot_rev'] - final_df['tot_rev_addons']) / final_df['guests']\n",
    "final_df['fee'] = final_df['fee'].round(2)\n",
    "\n",
    "## Creating 'final_dic' with all relevant dictionaries.\n",
    "final_dic = {'country': country_dic,\n",
    "             'us_region': us_region_dic, \n",
    "             'us_zipcode': us_zipcode_dic,\n",
    "             'firm_id': firm_id_dic,\n",
    "             'firm_type': firm_type_dic,\n",
    "             'status_cancel': status_cancel_dic,\n",
    "             'source_online': source_online_dic,\n",
    "             'payment_cc': payment_cc_dic,\n",
    "             'discount': discount_dic, \n",
    "             'q1_bef': q1_bef_dic, \n",
    "             'q1_aft': q1_aft_dic, \n",
    "             'q2': q2_dic, \n",
    "             'q3': q3_dic, \n",
    "             'q4': q4_dic, \n",
    "             'q5': q5_dic, \n",
    "             'q6': q6_dic, \n",
    "             'q7': q7_dic, \n",
    "             'addons': addons_dic}\n",
    "\n",
    "## Creating 'final_dic_pub' with all public dictionaries.\n",
    "final_dic_pub = {'country': country_dic,\n",
    "                 'us_region': us_region_dic, \n",
    "                 'us_zipcode': us_zipcode_dic,\n",
    "                 'firm_id': {'access denied': 'not publicly available'},\n",
    "                 'firm_type': {'access denied': 'not publicly available'},\n",
    "                 'status_cancel': status_cancel_dic,\n",
    "                 'source_online': source_online_dic,\n",
    "                 'payment_cc': payment_cc_dic,\n",
    "                 'discount': discount_dic, \n",
    "                 'q1_bef': q1_bef_dic, \n",
    "                 'q1_aft': q1_aft_dic, \n",
    "                 'q2': q2_dic, \n",
    "                 'q3': q3_dic, \n",
    "                 'q4': q4_dic, \n",
    "                 'q5': {'access denied': 'not publicly available'}, \n",
    "                 'q6': {'access denied': 'not publicly available'}, \n",
    "                 'q7': q7_dic, \n",
    "                 'addons': {'access denied': 'not publicly available'}}\n",
    "\n",
    "## Creating dataframe 'final_dic_df' from 'final_dic'.\n",
    "final_dic_df = pd.DataFrame.from_dict(final_dic)\n",
    "\n",
    "## Creating dataframe 'final_dic_pub_df' from 'final_dic_pub'\n",
    "final_dic_pub_df = pd.DataFrame.from_dict(final_dic_pub)\n",
    "\n",
    "print(final_df.info(verbose=True, null_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next code cell, we are going to:\n",
    "\n",
    "1-) Create directory 'data_new' where we are going to store csv files for analysis.  \n",
    "2-) Save 'final_df.csv' and 'final_df_dtypelist.csv' in the new directory.  \n",
    "3-) Save 'final_dic_df.csv' in the new directory.  \n",
    "4-) Save 'final_dic_pub_df.csv' in the new directory.\n",
    "\n",
    "'final_df.csv' and 'final_df_dtypelist.csv' will be used to recover 'final_df'.  \n",
    "'final_dic_df.csv' will be used to recover 'final_dic'.  \n",
    "'final_dic_pub_df.csv' will be used to recover 'final_dic_pub'.\n",
    "\n",
    "Attention: once recovered, 'final_dic' and 'final_dic_pub' will have all their items as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving csv files in 'data_new' for future analysis'.\n",
    "if os.path.isdir('data_new') == False:\n",
    "    os.mkdir('data_new')\n",
    "\n",
    "files = [(final_df, 'data_new/final_df.csv'), \n",
    "         (final_dic_df, 'data_new/final_dic_df.csv'), \n",
    "         (final_dic_pub_df, 'data_new/final_dic_pub_df.csv')]\n",
    "\n",
    "for file in files:\n",
    "    file[0].to_csv(path_or_buf=file[1])\n",
    "      \n",
    "final_df_dtypedic = {}\n",
    "for i in final_df.columns:\n",
    "    i_dtype = str(final_df[i].dtype)\n",
    "    final_df_dtypedic.update({i: i_dtype})\n",
    "final_df_dtypelist = list(final_df_dtypedic.items())\n",
    "\n",
    "with open('data_new/final_df_dtypelist.csv', 'w', newline='') as to_write:\n",
    "    writer = csv.writer(to_write)\n",
    "    writer.writerows(final_df_dtypelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next three code cells, we are going to demonstrate how to recover 'final_df', 'final_dic' and 'final_dic_pub' using the csv files in the 'data_new' directory . As said before, once recovered, 'final_dic' and 'final_dic_pub' will have all their items as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'new_final_df_dtypedic' equals to 'final_df_dtypedic'?\n",
      "True\n",
      "Is 'new_final_df' equals to 'final_df'?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Testing how to recover 'final_df'.\n",
    "new_final_df = pd.read_csv('data_new/final_df.csv', \n",
    "                           index_col=0, \n",
    "                           header=0, \n",
    "                           low_memory=False,\n",
    "                           keep_default_na=False,\n",
    "                           na_values='')\n",
    "\n",
    "with open('data_new/final_df_dtypelist.csv', newline='') as to_read:\n",
    "    reader = csv.reader(to_read)\n",
    "    data = list(reader)\n",
    "    new_final_df_dtypedic = dict(data)\n",
    "\n",
    "for i in new_final_df.columns:\n",
    "    new_final_df[i] = new_final_df[i].astype(new_final_df_dtypedic[i])\n",
    "\n",
    "print(\"Is 'new_final_df_dtypedic' equals to 'final_df_dtypedic'?\")\n",
    "print(new_final_df_dtypedic == final_df_dtypedic)\n",
    "print(\"Is 'new_final_df' equals to 'final_df'?\")\n",
    "print(new_final_df.equals(final_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'new_final_dic' equals to 'final_dic' with all items as string?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Testing how to recover 'final_dic'.\n",
    "new_final_dic_df = pd.read_csv('data_new/final_dic_df.csv',\n",
    "                               index_col=0, \n",
    "                               header=0, \n",
    "                               low_memory=False,\n",
    "                               keep_default_na=False,\n",
    "                               na_values='')\n",
    "\n",
    "new_final_dic = {}\n",
    "for key in new_final_dic_df.columns:\n",
    "    value = new_final_dic_df[key].dropna().to_dict()\n",
    "    new_final_dic.update({key: value})\n",
    "\n",
    "str_final_dic = {str(k): {str(i): str(j) for i, j in v.items()} \n",
    "                 for k, v in final_dic.items()}\n",
    "\n",
    "print(\"Is 'new_final_dic' equals to 'final_dic' with all items as string?\")\n",
    "print(new_final_dic == str_final_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'new_final_dic_pub' equals to 'final_dic_pub' with all items as string?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "## Testing how to recover 'final_dic_pub'.\n",
    "new_final_dic_pub_df = pd.read_csv('data_new/final_dic_pub_df.csv',\n",
    "                                   index_col=0, \n",
    "                                   header=0, \n",
    "                                   low_memory=False,\n",
    "                                   keep_default_na=False,\n",
    "                                   na_values='')\n",
    "\n",
    "new_final_dic_pub = {}\n",
    "for key in new_final_dic_pub_df.columns:\n",
    "    value = new_final_dic_pub_df[key].dropna().to_dict()\n",
    "    new_final_dic_pub.update({key: value})\n",
    "\n",
    "str_final_dic_pub = {str(k): {str(i): str(j) for i, j in v.items()} \n",
    "                     for k, v in final_dic_pub.items()}\n",
    "\n",
    "print(\"Is 'new_final_dic_pub' equals to 'final_dic_pub' with all items as string?\")\n",
    "print(new_final_dic_pub == str_final_dic_pub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
